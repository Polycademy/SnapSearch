<div class="documentation panel panel_lego panel_transition_yellow_dark">
    <div class="container">
        <div class="panel-heading">
            <h2 class="panel-title">Documentation</h2>
        </div>
        <div class="panel-body row">
            <div class="col-md-2">
                <nav class="btn-group-vertical documentation-nav" affix affix-top="214" affix-bottom="910">
                    <a class="btn" scroll="introduction">Introduction</a>
                    <a class="btn" scroll="apiUsage">API Usage</a>
                    <a class="btn" scroll="parameters">Parameters</a>
                    <a class="btn" scroll="middleware">Middleware</a>
                    <a class="btn" scroll="notes">Notes</a>
                </nav>
            </div>
            <div class="col-md-10">
                <div class="documentation-box">
                    <div class="documentation-information">
                        <h3 class="documentation-title" anchor="introduction">Introduction</h3>
                        <p>Snapsearch is a search engine optimisation (SEO) and robot proxy for complex front-end javascript & AJAX enabled (potentially realtime) HTML5 web applications.</p>
                        <p>Search engines like Google's crawler and dumb HTTP clients such as Facebook's image extraction robot cannot execute complex javascript applications. Complex javascript applications include websites that utilise AngularJS, EmberJS, KnockoutJS, Dojo, Backbone.js, Ext.js, jQuery, JavascriptMVC, Meteor, SailsJS, Derby, RequireJS and much more. Basically any website that utilises javascript in order to bring in content and resources asynchronously after the page has been loaded, or utilises javascript to manipulate the page's content while the user is viewing them such as animation.</p>
                        <p>Snapsearch intercepts any requests made by search engines or robots and sends its own javascript enabled robot to extract your page's content and creates a cached snapshot. This snapshot is then passed through your own web application back to the search engine, robot or browser.</p>
                        <p>Snapsearch's robot is an automated load balanced Firefox browser. This Firefox browser is kept up to date with the release, beta, alpha and nightly versions (beta, alpha and nightly is still under implementation), so we'll always be able to serve the latest in HTML5 technology. Our load balancer ensures your requests won't be hampered by other user's requests.</p>
                    </div>
                    <div class="documentation-information">
                        <h3 class="documentation-title" anchor="apiUsage">API Usage</h3>
                        <p>Make sure to register an account on SnapSearch before utilising these libraries. You will need your email and key to access SnapSearch's robot over SSL encrypted HTTP Basic Authorization.</p>
                        <p>SnapSearch can be easily integrated into your solution stack via our client middleware libraries. These client libraries first automatically detect if an HTTP request comes from a search engine or robot. If it is indeed a search engine, it sends an HTTP POST request to <code>https://snapsearch.io/api/v1/robot</code> passing in parameters configuring how SnapSearch's robot should extract your content. SnapSearch will then send a HTTP GET request to the same URL and return the HTTP response (status code, headers and content) as a JSON response to the library. The client library then returns that data back to your application. You will have to select which data to present to the search engine. It is recommended to return the status code and content but not all of the headers, due to potential header mismatch with content encoding. However if you have specific headers that are important, then first test if it works with a simple HTTP client before deploying it.</p>
                        <p>Our cache is dependent on your request parameters. So new request parameters will create a new independent snapshot.</p>
                        <p>All of the middleware libraries are ran on the application level requiring a server side programming language.</p>
                        <p>For specific usage and installation instructions navigate to specific middleware.</p>
                        <p>For sites using hash based url schemes please see our <a scroll="notes">notes on Hashbang urls</a>.</p>
                    </div>
                    <div class="documentation-information">
                        <h3 class="documentation-title" anchor="parameters">Request Parameters</h3>
                        <p>Our HTTPS Robot endpoint <code>/api/v1/robot</code> requires HTTP basic auth with your email and key as username and password respectively, and it accepts these parameters in POST or GET format:</p>
                        <dl>
                            <dt>url</dt>
                            <dd>
                                Url to scrape. This is the only required parameter. Every other parameter can be left as default.
                            </dd>
                            <dt>useragent</dt>
                            <dd>
                                Custom user agent. Default: Mozilla/5.0 (OPERATING SYSTEM) Gecko/VERSION Firefox/VERSION SnapSearch
                                <br /> 
                                Setting this user agent to something that does not include "SnapSearch" can cause an infinite interception loop.
                            </dd>
                            <dt>width</dt>
                            <dd>
                                Width of the Browser. Default: 1280px
                                <br />
                                Can be used if you have specific requirements regarding the dimensions of the browser.
                            </dd>
                            <dt>height</dt>
                            <dd>
                                Height of the Browser. Default: 1024px
                                <br />
                                Can be used if you have specific requirements regarding the dimensions of the browser.
                            </dd>
                            <dt>imgformat</dt>
                            <dd>
                                Image Format. Default: png
                                <br />
                                Is used in conjunction with <code>screenshot</code> parameter. Currently only supports png format. 
                            </dd>
                            <dt>screenshot</dt>
                            <dd>
                                Take a screenshot or not. Default: false
                                <br />
                                The screenshot is returned as base 64 encoded image and the format is determined by the <code>imgformat</code> parameter.
                            </dd>
                            <dt>navigate</dt>
                            <dd>
                                Follow redirects or not. Default: false
                                <br />
                                In most cases you do not want to follow redirects. If you leave this false, it will return the status, headers and body of the page asking for a redirect. If you switch this to true, it will follow header, client side, javascript, and meta tag redirects.
                            </dd>
                            <dt>loadimages</dt>
                            <dd>
                                Load images or not. Default: false
                                <br />
                                Loading images is not required when doing content scraping, leaving this off results in faster scrapes. However if you are taking screenshots, then you should switch this to true.
                            </dd>
                            <dt>javascriptenabled</dt>
                            <dd>
                                Process javascript or not. Default: true
                                <br />
                                Can be used in circumstances where you don't want to process javascript.
                            </dd>
                            <dt>totaltimeout</dt>
                            <dd>
                                Maximum millisecond timeout for the entire request task. Default 30000
                                <br />
                                This determines how long before the entire request task is considered a failure. At which point the robot will cancel the task and return everything it has managed to scrape.
                            </dd>
                            <dt>maxtimeout</dt>
                            <dd>
                                Maximum millisecond timeout for asynchronous requests. Default: 5000
                                <br />
                                This determines how long the browser will wait for asynchronous requests to finish. This means the browser will initiate the capture of the page contents either when all asynchronous requests finish, or at the maximum timeout. The number can be between 1000 to 15000 milliseconds. Longer times will result in potentially slower scrapes, but may capture more content if your site produces many slow asynchronous requests. If you set it too long, the client search engine robot may timeout. Play with this setting to the most optimal scraping speed.
                            </dd>
                            <dt>initialwait</dt>
                            <dd>
                                Initial millisecond wait before checking asynchronous requests. Default: 1000
                                <br />
                                This determines how long the browser will wait before it starts to check for when the asynchronous requests finish. The <code>maxtimeout</code> only begins onces the <code>initialwait</code> finishes. This is intended to allow delayed asynchronous requests or for pages which don't have asynchronous requests but have DOM mutations. The number has to be lower than <code>maxtimeout</code>.
                            </dd>
                            <dt>callback</dt>
                            <dd>
                                Javascript string to be evaled on the page. Default: false
                                <br />
                                This javascript string is evaled on the page after all asynchronous requests have finished but prior to the capture of the page contents. This allows you to do DOM mutations or capture specific content which will be returned as string independent of the page capture. You can assume this string is executed in the context of an anonymous function. So you can just write <code>return 'hello world';</code>. Returning string values is recommended, as other values may not be encoded. These returned values will be stored in the <code>callbackResult</code> property in the response.
                            </dd>
                            <dt>meta</dt>
                            <dd>
                                Scrape for custom meta tags or not. Default: true
                                <br />
                                You can use custom meta tags to change the status code or headers that is returned from the snapshot. This is intended for soft 404 techniques. It will look for meta tags such as <code>&lt;meta name=&quot;snapsearch-status&quot; content=&quot;404&quot; /&gt;</code> and <code>&lt;meta name=&quot;snapsearch-header&quot; content=&quot;Content-Type:text/html&quot; /&gt;</code>
                            </dd>
                            <dt>cache</dt>
                            <dd>
                                Allow caching of the snapshot or not. Default: true
                                <br />
                                This determines two things. The first is whether the snapshot can be acquired from the cache. The second is whether the snapshot can be cached. If this is left as true, snapshots can be returned from the cache if it exists in the cache or cached if it is a fresh snapshot. If this is switched to false, snapshots will always be fresh and the result will not be cached.
                            </dd>
                            <dt>cachetime</dt>
                            <dd>
                                Cache time in hours. Default: 24
                                <br />
                                You can change how long until the snapshots will expire. This number can be between 1 to 100 hours. Shorter cache time will result in more up to date snapshots, but it will use up more of your usage cap. Longer cache time will result in less up to date snapshots, but it will conserve your usage cap. This figure will depend on how often your pages change, what proportion of those pages are changing compared to the rest of the website.
                            </dd>
                            <dt>test</dt>
                            <dd>
                                Testing the API? Default: false
                                <br />
                                If you're in development mode or on localhost, set this to true, and it will only validate your request parameters, but not attempt to actually scrape anything.
                            </dd>
                        </dl>
                        <p>Here is an example response:</p>
                        <pre><code>[
    'cache'             =&gt; true/false,
    'callbackResult'    =&gt; '',
    'date'              =&gt; 1390382314,
    'headers'           =&gt; [
        [
            'name'  =&gt; 'Content-Type',
            'value' =&gt; 'text/html'
        ]
    ],
    'html'              =&gt; '&lt;html&gt;&lt;/html&gt;',
    'message'           =&gt; 'Success/Failed/Validation Errors',
    'pageErrors'        =&gt; [
        [
            'error'   =&gt; 'Error: document.querySelector(...) is null',
            'trace'   =&gt; [
                [
                    'file'      =&gt; 'filename',
                    'function'  =&gt; 'anonymous',
                    'line'      =&gt; '41',
                    'sourceURL' =&gt; 'urltofile'
                ]
            ]
        ]
    ],
    'screensot'         =&gt; 'BASE64 ENCODED IMAGE CONTENT',
    'status'            =&gt; 200
]</code></pre>
                        <p>We also record any javascript errors that occur on the page, these are stored in <code>pageErrors</code>, and it can be used for debugging custom <code>callback</code> parameters.</p>
                    </div>
                    <div class="documentation-information">
                        <h3 class="documentation-title" anchor="middleware">Middleware</h3>
                        <p>SnapSearch officially supports and provides PHP, Ruby, Node.js and Python middleware. All middleware are framework agnostic, and should be able to work within a middleware framework or without.</p>
                        <ul>
                            <li><a href="https://github.com/SnapSearch/SnapSearch-Client-PHP">PHP</a> - 1.1.0 (most stable)</li>
                            <li><a href="https://github.com/SnapSearch/SnapSearch-Client-Ruby">Ruby</a> - 0.1.0 (almost there)</li>
                            <li><a href="https://github.com/SnapSearch/SnapSearch-Client-Node">Node.js</a> - 0.0.2 (works but requires some improvements)</li>
                            <li><a href="https://github.com/SnapSearch/SnapSearch-Client-Python">Python</a> - Still under development.</li>
                        </ul>
                        <p>All of the middleware are open source, and we welcome pull requests for patches or new middleware implementations. Check out our <a href="https://github.com/SnapSearch/">Github organisation</a> for more.</p>
                    </div>
                    <div class="documentation-information">
                        <h3 class="documentation-title" anchor="notes">Notes</h3>
                        <h4>Hashbang Urls</h4>
                        <p>Make sure you are using hash bang urls and not just hash urls. This fits with <a href="https://developers.google.com/webmasters/ajax-crawling/docs/specification">Google's AJAX Crawling Scheme</a>. It makes it easier to identify what is a hash, and what is meant to be path. Remember hash fragments are never passed to the server. The middleware needs to know the full HTTP url or else it won't know where to scrape. This means you will need to rely on the search engine robots to convert hash bang urls to query fragment urls. <strong>This meta tag will need to be on every page: <code>&lt;meta name=&quot;fragment&quot; content=&quot;!&quot; /&gt;</code></strong>. If you are using HTML 5 push state urls, the meta tag is still a good practice as it allows search engines following the AJAX specification to know that your site is a single page application.</p>
                        <h4>Dealing with non-HTML resources</h4>
                        <p>You need to make sure that non-HTML resources are not intercepted by SnapSearch. Non-HTML resources refer to:</p>
                        <ul>
                            <li>Static files that are served through your application and not through an HTTP server such as NGINX or Apache.</li>
                            <li>Downloads that served through an application level controller.</li>
                            <li>Text data interchange formats that are not meant to be used for the end user's browser. For example: JSON, XML, RSS... etc.</li>
                            <li>API resources that don't display the front end site, but are there for interaction between machines.</li>
                            <li>Any connections that do not go through the HTTP protocol.</li>
                        </ul>
                        <p>You can prevent SnapSearch from intercepting these non-HTML resources by:</p>
                        <ul>
                            <li>Setup an array of whitelisted regular expression routes which will be matched against the request URL. SnapSearch will not intercept any routes that are not on the whitelist.</li>
                            <li>Setup an array of blacklisted regular expression routes which will be matched against the request URL. SnapSearch will not intercept any routes that are on the blacklist.</li>
                            <li>SnapSearch middleware can optionally check if the URL path has an invalid file extension. Some extensions are valid for HTML resources such as <code>.html</code>, but others such as <code>.js</code> are not. Our middleware has an option to switch on this detection and it will ignore requests that go invalid extensions. It is left off by default due to possible confusion.</li>
                            <li>In MVC style applications, you may have a single controller which is responsible for displaying the front end code. If you execute our clients inside that particular controller, then you will not have any problems with non-HTML resources, since it can only intercept requests that go to the front end.</li>
                        </ul>
                        <h4>SSL issues</h4>
                        <p>SnapSearch will not be able to capture from sites that have invalid SSL certificates (this means self-signed SSL certificates as well). Make sure your SSL is a valid certificate that will work in a normal browser before using SnapSearch.</p>
                        <h4>Ensuring Analytics Works with Snapsearch</h4>
                        <p>When Snapsearch visits your site, it will come with a UserAgent containing "SnapSearch". You can however configure this to your own liking. Use this user agent in order to filter out our requests when using web analytics.</p>
                        <h4>Large content</h4>
                        <p>SnapSearch middleware will timeout the request if the initial request to your site and its synchronous resources takes longer than 30 seconds.</p>
                        <h4>Supporting JS disabled Browsers</h4>
                        <p>It's not possible to detect if the HTTP client supports Javascript on the first page load. Therefore you have to know the user agents beforehand. A workaround involves the HTML Meta Refresh tag. You set an HTML meta refresh tag which will refresh the browser and point it to the same url but with query parameter indicating to the server that the client doesn't run javascript. This meta refresh tag can be then be cancelled using javascript. Another approach would be to use the Noscript tag and place the meta refresh tag there. None of these methods are guaranteed to work. but if you're interested check out: <a href="http://stackoverflow.com/q/3252743/582917">http://stackoverflow.com/q/3252743/582917</a></p>
                        <h4>Soft 404s</h4>
                        <p>Soft 404s should be avoided. The final representation to the search engine should be exactly the same as normal user with a normal browser would see. However you can achieve this by using the special meta tags and switching <code>meta</code> request parameter to true.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>